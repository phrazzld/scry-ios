# Scry - MVP Engineering Plan & Specifications

**Version:** 1.0-MVP
**Date:** 2025-04-13
**Status:** Final MVP Plan

## 1. Overview / Introduction

### 1.1. Vision & Goal
To create the most effortless and effective way for users to remember the things they care about, leveraging AI to eliminate friction in content creation and review scheduling. (MVP Focus: Establish core loop and generation pipeline).

### 1.2. Problem Statement
Traditional Spaced Repetition Systems (SRS) like Anki are powerful but suffer from high friction: tedious manual creation of learning material (cards), clunky user interfaces, and requiring significant user discipline to manage the review process.

### 1.3. Solution Overview (MVP)
Scry is an AI-enhanced, mobile-first SRS application MVP. Users capture simple **Memos** about what they want to remember. Scry's AI interprets these Memos and automatically generates relevant **Cards** behind the scenes. The core UX is a continuous, algorithmically prioritized stream of Cards for review, requiring minimal user interaction beyond engaging with the content.

### 1.4. Core Principles (MVP Focus)
* **Minimal Friction:** Eliminate unnecessary steps and cognitive load for the user (specifically removing generation confirmation for MVP).
* **Maximum Automation:** Leverage AI for content generation and scheduling.
* **Smooth UX:** Prioritize fluid transitions, intuitive interactions, and a polished feel.
* **Simple UI:** Focus on the core review loop, avoid clutter.
* **Mobile-First:** Design and build primarily for the mobile experience.
* **Focus on Remembering:** Validate the core loop's ability to aid retention.

## 2. Target Audience (MVP Focus)

* Early adopters willing to test an AI-driven learning tool.
* Users frustrated by Anki's card creation process.
* Individuals seeking a highly streamlined SRS experience.

## 3. Key Terminology (MVP)

* **Scry:** The name of the application.
* **Memo:** User-provided input (MVP: text only) describing what they want to remember. The raw material for AI processing.
* **Learning Objectives:** (Internal Concept) Structured interpretation of a Memo, generated and used internally by the AI pipeline to guide Card generation. Not user-facing in MVP.
* **Card:** A single, discrete unit of learning material presented to the user for review (MVP: Multiple Choice only). Generated by AI based on internal Learning Objectives derived from a Memo.
* **Review / Flow:** The core user experience of being presented with prioritized Cards for active recall practice.
* **SRS Algorithm:** The Spaced Repetition System algorithm used to determine the optimal time to review each Card (MVP: Basic implementation).

## 4. Core User Experience & Flows (MVP)

### 4.1. App Launch & Continuous Review Flow
1.  User opens Scry app.
2.  App immediately displays the highest-priority Card requiring review (based on SRS algorithm). No intermediate "dashboard" or "start session" screen.
3.  User interacts with the Card (answers Multiple Choice question).
4.  System provides immediate feedback (Correct/Incorrect).
5.  System records performance, updates Card's SRS state.
6.  App smoothly transitions to the next highest-priority Card.
7.  This loop continues as the default state.

### 4.2. Memo Capture & Card Generation Flow (MVP - Simplified)
1.  From the Review Flow, user activates "Add Memo" (e.g., via subtle persistent button, gesture). Review flow pauses.
2.  A simple text input interface is presented.
3.  User types their Memo text.
4.  User submits the Memo.
5.  System sends Memo text to backend. Backend acknowledges receipt (e.g., HTTP 202 Accepted) and queues an **asynchronous** Card generation task.
6.  User is returned *immediately* to the Review Flow where they left off.
7.  (Background Process): Backend AI pipeline processes Memo -> generates internal Learning Objectives -> generates Cards -> saves Cards & initial stats to DB.
8.  New Cards will enter the user's review pool based on SRS priority (initially, likely reviewed soon after creation) once generation is complete. No user confirmation step is required for MVP.

### 4.3. Review Interaction Flow (Answering & Actions - MVP)
1.  While viewing a Card in the Review Flow:
    * **Answering (MC - MVP):** User taps a choice. UI immediately highlights correct/incorrect selection. Backend logs outcome. Transition to next Card.
    * **Editing (MVP):** User activates "Edit". UI allows inline text editing of Card content (Question, Choices, Correct Answer designation). Saving updates the Card in DB. Return to viewing the Card or move to next.
    * **Deleting (MVP):** User activates "Delete". Confirmation prompt shown. If confirmed, Card (+ associated stats) is marked deleted/removed from DB. Transition to next Card.
    * **Postponing (MVP):** User activates "Postpone". System adjusts Card's `next_review_at` timestamp (e.g., pushes back by fixed duration/factor). Transition to next Card.

## 5. Functional Requirements (MVP)

* **FR1: User Authentication:** Basic email/password or OAuth signup/login.
* **FR2: Memo Input:** Allow users to input multi-line text as a Memo and submit it.
* **FR3: AI Card Generation Pipeline (MC):** System MUST asynchronously process a submitted Memo using AI (Gemini) to generate relevant Multiple Choice (MC) Cards. This includes internal interpretation (e.g., generating internal Learning Objectives) and formatting Cards correctly (question, choices, correct answer). This process MUST NOT require user intervention after Memo submission for MVP.
* **FR4: Core Review Loop:** App MUST immediately present the highest-priority Card upon launch/resume. Prioritization based on a functional SRS algorithm.
* **FR5: MC Card Interaction:** Users MUST be able to select an answer for an MC Card. System MUST provide immediate visual feedback on correctness.
* **FR6: SRS Algorithm (Basic):** Implement a basic SRS algorithm (e.g., SM-2 variant) to calculate `next_review_at` based on user performance (correct/incorrect) and store necessary stats (interval, ease factor).
* **FR7: Card Editing (Text):** Users MUST be able to edit the text content (question, choices) of a Card during review.
* **FR8: Card Deletion:** Users MUST be able to permanently delete a Card during review (with confirmation).
* **FR9: Card Postponing:** Users MUST be able to postpone the current Card, delaying its next review time via a simple mechanism.
* **FR10: Minimalist UI:** UI must be focused on the Card review, with other actions (Add Memo, Edit, Delete, Postpone) accessible but unobtrusive.

## 6. Non-Functional Requirements (NFRs - MVP Focus)

* **NFR1: Performance:**
    * App startup time < 2 seconds (cold).
    * Card-to-Card transition animation smooth (target 60fps) and completes < 300ms.
    * API response times < 500ms (p95) for core actions (fetching next card, submitting answer, submitting memo).
* **NFR2: Responsiveness:** UI must remain responsive during all operations. AI generation MUST be fully asynchronous and not block the UI thread or core API responses.
* **NFR3: Scalability (MVP Focus):** Database schema designed with essential indexing for core queries (fetching next card for user). Backend designed to be stateless. Handle concurrent users gracefully (MVP target: low hundreds).
* **NFR4: Reliability:** Handle API errors (LLM, internal) gracefully (e.g., log errors, potentially mark generation as failed). Basic data validation on inputs. Aim for >99% uptime for core review functionality.
* **NFR5: Usability:** Interface must be highly intuitive for the core loop ("dummy simple"). Minimize cognitive load.
* **NFR6: Security:** Secure authentication (store passwords hashed/salted). Use HTTPS for all communication. Protect API keys.
* **NFR7: Data Integrity:** Ensure user actions (answers, edits, deletes) and generated cards are reliably persisted.

## 7. Architecture (MVP)

### 7.1. High-Level Diagram
```
+-----------------+      +----------------+      +-----------------+      +-----------------+
| React Native App| <--> |   Go Backend   | <--> |   PostgreSQL    |      |   Gemini API    |
| (Scry Frontend) |      |    (API)       |      | (+ pgvector)    | <--> | (LLM Provider)  |
+-----------------+      +----------------+      +-----------------+      +-----------------+
```

### 7.2. Frontend Architecture (React Native - MVP)
* **Language:** TypeScript
* **Core Libraries:** React Native, React Navigation, State Management (e.g., Zustand), API Client (e.g., Axios), Reanimated.
* **Key Components:** `ReviewScreen`, `CardView`, `MemoInputModal`, `EditCardModal`.
* **State Management:** Manage current card queue, user session, UI state for core loop.

### 7.3. Backend Architecture (Go - MVP)
* **Language:** Go
* **Framework/Router:** Standard library + Router (e.g., `chi`)
* **Key Packages/Services:** `api`, `handlers`, `srs`, `llm`, `store`, `tasks`, `domain`, `auth`.
* **AI Processing Flow:** `POST /memos` handler receives text -> Enqueues job `ProcessMemo(memoText, userId)` -> Task runner picks up job -> Calls `llmService.GenerateCardsFromMemo(memoText)` -> `llmService` internally calls Gemini (potentially multi-step: text -> objectives -> cards) -> Parses results -> Calls `store` to save Cards and initial `user_card_stats`.
* **API Endpoints (MVP):**
    * `POST /auth/register`, `POST /auth/login`
    * `POST /memos` (input Memo text) -> returns HTTP 202 Accepted.
    * `GET /cards/next` (gets highest priority card)
    * `POST /cards/{id}/answer` (submits answer)
    * `PUT /cards/{id}` (edits card content)
    * `DELETE /cards/{id}`
    * `POST /cards/{id}/postpone`
* **Concurrency:** Utilize Go's concurrency for handling requests and background generation tasks.

### 7.4. Database Schema (PostgreSQL + pgvector - MVP)
* **Extension:** `vector` enabled (for potential future use, embedding generation deferred post-MVP unless trivial).
* **Tables (MVP Focus):**
    * `users` (`user_id` UUID PK, `email` TEXT UNIQUE NOT NULL, `password_hash` TEXT NOT NULL, `created_at` TIMESTAMPTZ, `updated_at` TIMESTAMPTZ)
    * `memos` (`memo_id` UUID PK, `user_id` UUID FK NOT NULL, `input_text` TEXT NOT NULL, `generation_status` TEXT NOT NULL default 'pending', /* 'pending', 'processing', 'completed', 'failed' */ `created_at` TIMESTAMPTZ, `updated_at` TIMESTAMPTZ)
    * `cards` (`card_id` UUID PK, `user_id` UUID FK NOT NULL, `source_memo_id` UUID FK NULL, `card_type` TEXT NOT NULL default 'mc', `content` JSONB NOT NULL /* { question: "...", choices: [...], correct_index: N } */, `embedding` VECTOR(embedding_dim) NULL, `created_at` TIMESTAMPTZ, `updated_at` TIMESTAMPTZ)
    * `user_card_stats` (`user_id` UUID FK NOT NULL, `card_id` UUID FK NOT NULL, `next_review_at` TIMESTAMPTZ NOT NULL, `last_reviewed_at` TIMESTAMPTZ NULL, `last_outcome` TEXT NULL, `current_interval_seconds` INTEGER NOT NULL default 0, `ease_factor` FLOAT NOT NULL default 2.5, `created_at` TIMESTAMPTZ, `updated_at` TIMESTAMPTZ, PRIMARY KEY (`user_id`, `card_id`))
* **Key Indexes (MVP):** Index on `user_card_stats (user_id, next_review_at)`, Index on `cards (user_id)`.

### 7.5. AI Integration (Gemini API - MVP)
* **Models:** Select cost-effective Gemini model(s) (e.g., Flash) suitable for the Memo -> MC Card generation task.
* **Prompt Engineering (MVP Focus):** Develop robust prompt template(s) for the end-to-end task: Convert raw `Memo` text directly into formatted MC `Cards`. This might involve internal chain-of-thought prompting or few-shot examples within the prompt to guide the AI through interpretation (internal objectives) and formatting. Emphasis on getting usable MC cards directly from the Memo reliably.
* **API Client:** Go client library for Gemini API.
* **Error Handling:** Handle LLM API errors during background generation; mark Memo/Cards appropriately if generation fails.

### 7.6. Infrastructure (MVP - Initial Choice: DigitalOcean / Supabase)
* **Hosting:** App Service / Droplets (DO) or Managed Service (Supabase) for Go Backend.
* **Database:** Managed PostgreSQL instance (DO or Supabase) with `pgvector` available.
* **Frontend Distribution:** App Stores (iOS TestFlight/App Store, Android Play Store via Expo Application Services or similar).
* **Task Queue:** Simple in-memory Go channels + goroutines suitable for MVP load.

## 8. MVP Feature Specifications (Detailed - Examples)

* **Spec-1: Memo Submission & Async Trigger:**
    * UI: Modal with multi-line text input, Submit button. On Submit, display transient success indicator (e.g., "Memo added! Generating cards...") and return user to Review Flow.
    * Backend (`POST /memos`): Takes `{inputText: string}`. Validates input. Stores Memo with `inputText`, status `pending`. Enqueues background job `GenerateCardsFromMemo(memoId, inputText, userId)`. Returns HTTP 202 Accepted.
* **Spec-2: Async Card Generation (MVP):**
    * Background Task (`GenerateCardsFromMemo`): Fetches Memo details. Calls `llmService` with "Generate MC Cards From Memo" prompt using `inputText`. Parses LLM response containing multiple cards. For each card: validates format, saves Card to DB (associating with `source_memo_id`), initializes `user_card_stats` with `next_review_at = NOW()`. Updates Memo status to `completed` or `failed`. Handles LLM/parsing errors.
* **Spec-3: Fetch Next Card:** (Same as previous plan)
* **Spec-4: Answer Card & SRS Update:** (Same as previous plan)

## 9. Open Questions & Risks (MVP Focus)

* **AI Quality (Direct Memo->Card):** Can the AI reliably generate good MC Cards directly from unstructured Memos without the explicit Objective confirmation step? This is the core risk increased by simplifying the flow. Requires significant prompt engineering and testing.
* **AI Cost Management:** Costs remain a factor, potentially higher per Memo if more complex prompts are needed for direct Card generation.
* **"Black Box" Generation:** Users have less control/visibility if generation goes off track. Need good Edit/Delete tools.
* **Cold Start:** How engaging is the app before the user adds their first Memos and waits for generation? (Consider pre-populating with example Cards or having a very clear "Add your first Memo!" prompt).
* **UX Smoothness:** Achieving seamless async updates (new cards appearing) and transitions remains a core challenge.

## 10. Naming Conventions (Recap)

* **App:** Scry
* **Input:** Memo
* **Review Unit:** Card
* **Code:** Go (snake_case packages, CamelCase functions/types), TypeScript (camelCase variables/functions, PascalCase types/components).
* **Database:** PostgreSQL (snake_case tables/columns).

## 11. Success Metrics (MVP Focus)

* **Activation:** % of signups who create their first Memo within 24h. % of first Memos that successfully generate cards.
* **Core Loop Engagement:** DAU, Cards reviewed per DAU, Session length/frequency.
* **Content Generation:** Memos created per active user, User Card deletion rate (as proxy for generation quality).
* **Retention:** Day 1, Day 7 retention rates.
* **Technical:** API uptime, p95 response times, app crash rate, Card generation success rate & latency.
* **Qualitative:** User feedback specifically on the quality/relevance of generated cards and the ease of the Memo -> Card flow.
